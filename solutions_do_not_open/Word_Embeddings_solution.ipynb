{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zerotodeeplearning/ztdl-masterclasses/blob/master/solutions_do_not_open/Word_Embeddings_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bwH96hViwS7"
   },
   "source": [
    "## Learn with us: www.zerotodeeplearning.com\n",
    "\n",
    "Copyright © 2021: Zero to Deep Learning ® Catalit LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFidPKNdkVPg"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvoukA2tkGV4"
   },
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9yGuqMlt3uv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/zerotodeeplearning/ztdl-masterclasses/master/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Li0E1D-X0lS6"
   },
   "outputs": [],
   "source": [
    "pos_path = tf.keras.utils.get_file(\n",
    "    'rotten_tomatoes_positive_reviews.txt',\n",
    "    url + 'rotten_tomatoes_positive_reviews.txt.gz',\n",
    "    extract=True)\n",
    "neg_path = tf.keras.utils.get_file(\n",
    "    'rotten_tomatoes_negative_reviews.txt',\n",
    "    url + 'rotten_tomatoes_negative_reviews.txt.gz',\n",
    "    extract=True)\n",
    "\n",
    "with gzip.open(pos_path) as fin:\n",
    "  pos_rev = fin.readlines()\n",
    "  pos_rev = [r.decode('utf-8') for r in pos_rev]\n",
    "\n",
    "with gzip.open(neg_path) as fin:\n",
    "  neg_rev = fin.readlines()\n",
    "  neg_rev = [r.decode('utf-8') for r in neg_rev]\n",
    "  \n",
    "docs = np.array(pos_rev + neg_rev)\n",
    "y = np.array([1]*len(pos_rev) + [0]*len(neg_rev))\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(docs, y, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence encoding with Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCFnHoTw4nWp"
   },
   "outputs": [],
   "source": [
    "max_features = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JH_WNp-X0ya4"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    num_words=max_features,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`\\'{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    document_count=0,\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1G3gO-Wa2YjN"
   },
   "outputs": [],
   "source": [
    "seq_train = tokenizer.texts_to_sequences(docs_train)\n",
    "seq_test =tokenizer.texts_to_sequences(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([tokenizer.index_word[i] for i in seq_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(s) for s in seq_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(s) for s in seq_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFVrp-c22bH7"
   },
   "outputs": [],
   "source": [
    "maxlen=58\n",
    "\n",
    "X_train = pad_sequences(seq_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(seq_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of word model with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K95xWBS4eDk"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "IaAuaQqW4Z3O",
    "outputId": "2b1fae8f-b4be-48d5-ca90-6e23f8edc67a"
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(max_features,\n",
    "            embedding_dim,\n",
    "            input_length=maxlen,\n",
    "            name='bow_embeddings'),\n",
    "  Dropout(0.3),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(24, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "iFO42vDv4u70",
    "outputId": "f7ed07da-044f-4eee-e8ae-4e353a2af119"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "h = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=4,\n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(h.history).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "The model above is still a bag of words model, despite the use of embeddings. Let's improve it using 1D convolutional layers.\n",
    "\n",
    "- Define a new `Sequential` model that uses `Conv1D` layers after the `Embedding` layer\n",
    "- Start with the simplest model possible and gradually increase the complexity\n",
    "- Train the model as above and compare the performance of this model with the previous one\n",
    "\n",
    "Your code will look like:\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "  Embedding(# YOUR CODE HERE\n",
    "  # YOUR CODE HERE\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "empty"
    ]
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Embedding(max_features, embedding_dim, input_length=maxlen),\n",
    "  Conv1D(32, 3, activation='relu'),\n",
    "  Conv1D(64, 3, activation='relu'),\n",
    "  Flatten(),\n",
    "  Dense(24, activation='relu'),\n",
    "  Dropout(0.3),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "h = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=4,\n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(h.history).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim and pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = api.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['models'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['good'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['two'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['king', 'woman'],\n",
    "                         negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_size = len(glove_model['cat'])\n",
    "glove_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_weights = np.zeros(shape=(max_features, glove_size))\n",
    "for i in range(1, max_features):\n",
    "  w = tokenizer.index_word[i]\n",
    "  try:\n",
    "    v = glove_model[w]\n",
    "    glove_weights[i] = v\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(glove_model['two'])\n",
    "plt.plot(glove_model['three'])\n",
    "plt.plot(glove_model['four'])\n",
    "plt.title(\"A few numbers\")\n",
    "plt.ylim(-2, 5)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(glove_model['cat'])\n",
    "plt.plot(glove_model['dog'])\n",
    "plt.plot(glove_model['rabbit'])\n",
    "plt.title(\"A few animals\")\n",
    "plt.ylim(-2, 5)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Let's use the Glove pre-trained embeddings as our input layer.\n",
    "\n",
    "- Modify the Embedding layer in your model using a `Constant` initializer that sets the weights to be `glove_weight`\n",
    "- Adapt the `output_dim` to correspond to the size of glove embeddings\n",
    "- Set the Embedding layer to be frozen (`trainable=False`)\n",
    "- Re-train the model and compare the performance\n",
    "\n",
    "Your code will look like:\n",
    "```python\n",
    "model = Sequential([\n",
    "  Embedding(# YOUR CODE HERE\n",
    "  # YOUR CODE HERE\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "empty"
    ]
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Embedding(input_dim=max_features,\n",
    "            output_dim=glove_size,\n",
    "            embeddings_initializer=Constant(glove_weights),\n",
    "            input_length=maxlen,\n",
    "            mask_zero=False,\n",
    "            trainable=False),\n",
    "  Conv1D(128, 5, activation='relu'),\n",
    "  MaxPooling1D(5),\n",
    "  Conv1D(128, 5, activation='relu'),\n",
    "  GlobalMaxPooling1D(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dropout(0.3),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "h = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=8,\n",
    "    validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "authorship_tag": "ABX9TyOJWB9s6S9wS5YLk8la7mLZ",
   "include_colab_link": true,
   "name": "Word_Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
