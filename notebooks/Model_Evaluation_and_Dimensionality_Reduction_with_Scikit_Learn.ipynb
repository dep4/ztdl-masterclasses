{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zerotodeeplearning/ztdl-masterclasses/blob/master/notebooks/Model_Evaluation_and_Dimensionality_Reduction_with_Scikit_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bwH96hViwS7"
   },
   "source": [
    "## Learn with us: www.zerotodeeplearning.com\n",
    "\n",
    "Copyright © 2021: Zero to Deep Learning ® Catalit LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFidPKNdkVPg"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvoukA2tkGV4"
   },
   "source": [
    "# Model Evaluation and Dimensionality Reduction with Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "gnxKmzv_ryWh",
    "outputId": "f2ab1ae6-8ddf-4c58-f05d-ff9558266d25"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPeOozeoy-FX"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/zerotodeeplearning/ztdl-masterclasses/master/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUkgzFEAzBmu"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url + 'sms.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "B3r2u7-TzIpv",
    "outputId": "8fcc0d80-3f54-4b20-8376-ce05cc6e5688"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "359lUlDPzf4x",
    "outputId": "985fd7f8-177d-42ac-e6ba-fb7aea131ac0"
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pK70Ebq2zP5k"
   },
   "outputs": [],
   "source": [
    "y = (df['label'] == 'spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIZRqPwD2jqH"
   },
   "source": [
    "### Word count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsFl9ktm2nRa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxCsXq8vtvP_"
   },
   "outputs": [],
   "source": [
    "def cross_val_score_print(model, X, y, cv=3):\n",
    "    scores = cross_val_score(model, X, y, cv=cv)\n",
    "    print(\"Accuracy score: {:0.3} +/- {:0.3}\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr9475xD2o4z"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(decode_error='ignore',\n",
    "                       stop_words='english',\n",
    "                       binary=True,\n",
    "                       max_features=2000)\n",
    "\n",
    "X = vect.fit_transform(df['msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aTcUkH6t2rOq",
    "outputId": "ecc6674a-281c-4887-b10f-e4bf0ddc88af"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hz1jJQNgxcM_"
   },
   "source": [
    "Visualize the first 200 word counts in the first 200 messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "colab_type": "code",
    "id": "BP1JVlwbv2Pq",
    "outputId": "32c8aa82-b3ab-4b1e-97bc-f2d3ceffe272"
   },
   "outputs": [],
   "source": [
    "N = 200\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(X.todense()[:N, :N]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "urrahefmt_2D",
    "outputId": "32df48e7-da02-4ab0-c8b5-754080045fc7"
   },
   "outputs": [],
   "source": [
    "cross_val_score_print(DummyClassifier(strategy='most_frequent'), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WXyHcmJf2sVO",
    "outputId": "15731754-2ba9-4687-cd97-e9ea0820c9e7"
   },
   "outputs": [],
   "source": [
    "cross_val_score_print(LogisticRegression(solver='liblinear'), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywuPzqWF2tdz"
   },
   "source": [
    "### Feature importances\n",
    "\n",
    "The model using 2000 word features seems to be performing quite well. Let's find out which words are more correlated with `Spam`.\n",
    "\n",
    "The features we are using are counts of word occurrences in a corpus of SMS messages. Since SMSs have a fixed length, we can assume that these counts are proportional to the fequencies of occurrences. In other words we can assume that all the features have the same normalization scale. Under this assumption, we can interpret the coefficients of the `LogisticRegression` model as features importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "7erw23Ez2xOI",
    "outputId": "a6755f02-bb3c-4251-b71d-0692cc09eefd"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olDexqIF20Ch"
   },
   "outputs": [],
   "source": [
    "word_feature_importances = pd.Series(model.coef_[0],\n",
    "                                     index=vect.get_feature_names()).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "Fbkok5Qr21_V",
    "outputId": "8f3f4b07-fd28-41f2-86e8-17f80a249dcc"
   },
   "outputs": [],
   "source": [
    "# Top 20 least spammy words\n",
    "word_feature_importances.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "fxSPWTMK23_M",
    "outputId": "e51ed1d9-f6ca-45d4-ce91-e3459dc07496"
   },
   "outputs": [],
   "source": [
    "# Top 20 most spammy words\n",
    "word_feature_importances.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKZO63DwyK06"
   },
   "source": [
    "### Truncated SVD\n",
    "\n",
    "A common way to visualize highly dimensional feature sets is to use the Truncated SVD dimensionality reduction technique. Let's use it to compress our 2000 sparse features to a 5 dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OBDJ4g4ySvJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RnFUXq-yS0X"
   },
   "outputs": [],
   "source": [
    "X_tsvd = pd.DataFrame(TruncatedSVD(n_components=5).fit_transform(X), columns=['c1', 'c2', 'c3', 'c4', 'c5'])\n",
    "X_tsvd['label'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "colab_type": "code",
    "id": "CPQ4eWrMyS3m",
    "outputId": "65fd9536-b00b-4e27-ac1e-345524bee238"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(X_tsvd, hue='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNjf-Bi1mhN9"
   },
   "source": [
    "### Exercise 1: Model validation with Pipelines.\n",
    "\n",
    "When the feature engineering step involves learning something from the data we should only learn from the training set.\n",
    "\n",
    "In the case above, we are learning the vocabulary from the data, but there are many other cases where the transformer is learning  properties of the data. In these cases, we should proceed with caution and only learn from the training set.\n",
    "\n",
    "One way to achieve this is to do something like this:\n",
    "\n",
    "```python\n",
    "raw_features_train, raw_features_test = train_test_split(....)\n",
    "transformer = ....\n",
    "\n",
    "transformer.fit(raw_features_train)\n",
    "X_train = transformer.transform(raw_features_train)\n",
    "X_test = transformer.transform(raw_features_test)\n",
    "```\n",
    "\n",
    "a better way to achieve the same is to bundle the transformer and the estimator into a [`Pipeline`](https://scikit-learn.org/stable/modules/compose.html).\n",
    "\n",
    "Complete the following steps:\n",
    "\n",
    "- Split `df['msg']` and `y` into train and test sets\n",
    "- Create a pipeline using the `make_pipeline` function that contains at least 2 steps: `vect` and `LogisticRegression()`. Feel free to include additional intermediate steps if you wish\n",
    "- Train the pipeline model on the trainin set and compare the training and test scores\n",
    "- Bonus points if you perform Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNUJFzJDEoFe"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AHCiSoj2cji"
   },
   "source": [
    "### Exercise 2: ROC curve and learning curve\n",
    "\n",
    "- Use the trained pipeline model to calculate the [`roc_curve`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)\n",
    "- Bonus point if you plot it for both train and test sets defined above.\n",
    "- Use the pipeline model to calculate and plot the [`learning_curve`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYVZqPsq1ya2"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Let's explore the effect of Dimensionality Reduction techniques on a different dataset: the Digits dataset.\n",
    "\n",
    "The data is loaded for you.\n",
    "\n",
    "- Use one or more dimensionality reduction techniques (e.g. `PCA`, `TSNE` or other) to compress the 64 pixel features into 2 features.\n",
    "- Use `sns.scatterplot` to visualize the whole dataset in the reduced space\n",
    "- Use the `y` variable to color the data: do you see clusters of similar points appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5LnQ1MxpBiOT"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkuIrpu7Bpf_"
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6byMkbOUBuPG",
    "outputId": "4541b70d-1aac-435a-8468-d0b278202cf7"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "UfzpZjlXBuMX",
    "outputId": "bb5cbd1e-8348-4ad4-ddf9-03206b16cc12"
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[0].reshape(8, 8), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "htmtdb_MBuJq",
    "outputId": "e88ea7c3-4205-4239-82a9-166f07bd6e04"
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[1].reshape(8, 8), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJWB9s6S9wS5YLk8la7mLZ",
   "include_colab_link": true,
   "name": "Model_Evaluation_and_Dimensionality_Reduction_with_Scikit_Learn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
