{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zerotodeeplearning/ztdl-masterclasses/blob/master/notebooks/Attention_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bwH96hViwS7"
   },
   "source": [
    "## Learn with us: www.zerotodeeplearning.com\n",
    "\n",
    "Copyright © 2021: Zero to Deep Learning ® Catalit LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFidPKNdkVPg"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvoukA2tkGV4"
   },
   "source": [
    "# Attention with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeAWZNIdxFI3"
   },
   "source": [
    "This exercise follows:\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usTEmKM7kdVl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, MultiHeadAttention, Dense, GlobalAveragePooling1D, Dropout, Dense, LayerNormalization, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the IMDB dataset and its word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfN9FD8mkqp-",
    "outputId": "65b818d8-c761-4dbc-cce4-c20100cefcd5"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "maxlen = 200\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "inverted_word_index = dict((i+3, word) for (word, i) in word_index.items())\n",
    "inverted_word_index[0] = ''\n",
    "inverted_word_index[1] = '<start>'\n",
    "inverted_word_index[2] = '<oov>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5tkLay9ksg6",
    "outputId": "88c45658-b4a4-4d27-8b48-6865fc96140b"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuoO5CgAlLIz",
    "outputId": "166ecfcc-5d83-4c7f-f3d1-8ea1ecc4c03e"
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a couple of sentences for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "hrdoWLJhlM2c",
    "outputId": "25085b4b-f948-46fc-b2dc-86e6177a5268"
   },
   "outputs": [],
   "source": [
    "\" \".join(inverted_word_index[i] for i in X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "NM03LbYWlj00",
    "outputId": "99e7673a-1719-4e83-ca01-e76c3efaaf2f"
   },
   "outputs": [],
   "source": [
    "\" \".join(inverted_word_index[i] for i in X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9thqY9anz8g"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "  def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "    super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "    self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "    self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "  def call(self, x):\n",
    "    maxlen = tf.shape(x)[-1]\n",
    "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "    positions = self.pos_emb(positions)\n",
    "    x = self.token_emb(x)\n",
    "    return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display a few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Amo9V3vpoRF_"
   },
   "outputs": [],
   "source": [
    "example_tpe = Sequential([TokenAndPositionEmbedding(maxlen, vocab_size, 32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rn1rrjXyrOOX"
   },
   "outputs": [],
   "source": [
    "n_reviews = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZYDWHBOoW8n"
   },
   "outputs": [],
   "source": [
    "embedded_sentences = example_tpe(X_train[:n_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "2HTHE3BCq_m_",
    "outputId": "e9c859d0-d614-4e40-b56f-c6b2b39d56de"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(n_reviews):\n",
    "  plt.subplot(n_reviews, 1, i+1)\n",
    "  plt.imshow(embedded_sentences.numpy()[i].transpose())\n",
    "  plt.xlabel(\"word in sentence -->\")\n",
    "  plt.ylabel(\"<-- embedding dim\")\n",
    "  plt.title(f\"movie review {i}\")\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iivbLxlSolvb"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "  def __init__(self, embed_dim, n_att_heads, n_dense_nodes, rate=0.1, **kwargs):\n",
    "    super(TransformerBlock, self).__init__(**kwargs)\n",
    "    self.att = MultiHeadAttention(num_heads=n_att_heads, key_dim=embed_dim)\n",
    "    self.ffn = Sequential([\n",
    "        Dense(n_dense_nodes, activation=\"relu\"),\n",
    "        Dense(embed_dim)]\n",
    "    )\n",
    "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    self.dropout1 = Dropout(rate)\n",
    "    self.dropout2 = Dropout(rate)\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    attn_output = self.att(inputs, inputs)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(inputs + attn_output)\n",
    "    ffn_output = self.ffn(out1)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using either the Sequential or the Functional API in Keras build a transformer classification model with the following architecture:\n",
    "\n",
    "```\n",
    "    TokenAndPositionEmbedding(...\n",
    "    TransformerBlock(...\n",
    "    GlobalAveragePooling1D(...\n",
    "    Dropout(...\n",
    "    Dense(...\n",
    "    Dropout(...\n",
    "    Dense(2, activation=\"softmax\")\n",
    "````\n",
    "\n",
    "Once the model is built, print out the summary.\n",
    "\n",
    "You will need to decide a few hyperparameters including:\n",
    "\n",
    "- Embedding size\n",
    "- Number of attention heads\n",
    "- Size of the dense hidden layer inside the transformer block\n",
    "- Size of the other dense layers\n",
    "- Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Compile, train, and evaluate the model. Pay attention to the loss function. We defined the output layer as a `Dense(2, activation=\"softmax\")` so you will need to choose the loss accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJWB9s6S9wS5YLk8la7mLZ",
   "include_colab_link": true,
   "name": "Attention_with_Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
